{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations with IBM\n",
    "\n",
    "In this notebook, I will be applying my recommendation skills on real data from the IBM Watson Studio platform.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "I. [Exploratory Data Analysis](#Exploratory-Data-Analysis)<br>\n",
    "II. [Rank Based Recommendations](#Rank)<br>\n",
    "III. [User-User Based Collaborative Filtering](#User-User)<br>\n",
    "IV. [Content Based Recommendations](#Content-Recs)<br>\n",
    "V. [Matrix Factorization](#Matrix-Fact)<br>\n",
    "VI. [Extras & Concluding](#conclusions)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "\n",
    "import tests.project_tests as t\n",
    "\n",
    "nltk.download(['punkt', 'wordnet', 'stopwords', 'averaged_perceptron_tagger', 'punkt_tab'])\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style(\"whitegrid\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dtype_dict = {'article_id': int}\n",
    "df = pd.read_csv('../data/user_item_interactions.csv', dtype=dtype_dict)\n",
    "df_content = pd.read_csv('../data/articles_community.csv')\n",
    "del df['Unnamed: 0']\n",
    "del df_content['Unnamed: 0']\n",
    "\n",
    "# Show df to get an idea of the data\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Show df_content to get an idea of the data\n",
    "df_content.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_content.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_content.nunique()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.nunique()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"Exploratory-Data-Analysis\">Part I : Exploratory Data Analysis</a>\n",
    "\n",
    "`1.` What is the distribution of how many articles a user interacts with in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "article_distribution = pd.DataFrame(df.groupby(['email'])['article_id'].size())\n",
    "article_distribution.columns = ['document_count']\n",
    "article_distribution.sort_values(by='document_count',\n",
    "                                 ascending=False,\n",
    "                                 inplace=True)\n",
    "article_distribution.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# descriptive statistics\n",
    "article_distribution.describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# visual statistics\n",
    "plt.figure(figsize=(14, 6))\n",
    "avg_count = round(article_distribution['document_count'].mean(), 0)\n",
    "ax = sns.stripplot(x='email',\n",
    "                   y='document_count',\n",
    "                   data=article_distribution,\n",
    "                   jitter=False)\n",
    "ax.axhline(avg_count, color='r', linestyle='--')\n",
    "plt.text(avg_count + 300,\n",
    "         avg_count, 'Mean: {}'.format(avg_count),\n",
    "         fontsize=10,\n",
    "         horizontalalignment='right',\n",
    "         verticalalignment='top')\n",
    "ax.set(xticklabels=[])\n",
    "ax.set(xlabel='Users')\n",
    "ax.set(title='Distribution of articles a user interacts with')\n",
    "ax.set(ylabel='Number of articles read')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Observations:** <br>\n",
    "On average, users interact with 9 documents, which seems quite low.\n",
    "The top 25% of users have much higher interactions with up to 350 documents."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Fill in the median and maximum number of user_article interactions below\n",
    "# 50% of individuals interact with ____ number of articles or fewer.\n",
    "median_val = 3\n",
    "\n",
    "# The maximum number of user-article interactions by any 1 user is ______.\n",
    "max_views_by_user = 364"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "`2.` Explore and remove duplicate articles from the **df_content** dataframe."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print('There are {} duplicate articles'.format(\n",
    "    df_content.article_id.duplicated().sum()))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Remove any rows that have the same article_id - only keep the first record\n",
    "df_content.drop_duplicates(['article_id'], keep='first', inplace=True)\n",
    "print('There are {} duplicate articles'.format(\n",
    "    df_content.article_id.duplicated().sum()))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Are there documents with duplicate titles?\n",
    "print('There are {} duplicate articles titles'.format(\n",
    "    df_content.doc_full_name.duplicated().sum()))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# extra checks for null values in df_content\n",
    "df_content.isnull().sum()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# In addition, remove null values from df_contents\n",
    "df_content.dropna(subset=['doc_description', 'doc_body'], inplace=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` Use the cells below to find:\n",
    "\n",
    "**a.** The number of unique articles that have an interaction with a user.  \n",
    "**b.** The number of unique articles in the dataset (whether they have any interactions or not).<br>\n",
    "**c.** The number of unique users in the dataset. (excluding null values) <br>\n",
    "**d.** The number of user-article interactions in the dataset."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**a.** The number of unique articles that have an interaction with a user."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.article_id.nunique()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**b.** The number of unique articles in the dataset (whether they have any interactions or not).<br>"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# b. The number of unique articles in the dataset\n",
    "# (whether they have any interactions or not)\n",
    "df_content.article_id.nunique()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**c.** The number of unique users in the dataset. (excluding null values) <br>"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.email.nunique()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**d.** The number of user-article interactions in the dataset."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.shape",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "unique_articles = 714  # The number of articles with interactions\n",
    "total_articles = 1051  # Number of total articles\n",
    "unique_users = 5148  # The number of unique users\n",
    "user_article_interactions = 45993  # The number of user-article interactions"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4.` Use the cells below to find the most viewed **article_id**, as well as how often it was viewed.  After talking to the company leaders, the `email_mapper` function was deemed a reasonable way to map users to ids.  There were a small number of null values, and it was found that all of these null values likely belonged to a single user (which is how they are stored using the function below)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# display the top 10 viewed articles\n",
    "df.article_id.value_counts()[:10]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# The most viewed article in the dataset\n",
    "# as a string with one value following the decimal\n",
    "most_viewed_article_id = '1429.0'\n",
    "\n",
    "# The most viewed article in the dataset was viewed how many times?\n",
    "max_views = 937"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Udacity Evaluation"
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# If you stored all your results in the variable names above,\n",
    "# you shouldn't need to change anything in this cell\n",
    "\n",
    "sol_1_dict = {\n",
    "    '`50% of individuals have _____ or fewer interactions.`': median_val,\n",
    "    '`The total number of user-article interactions in the dataset is ______.`': user_article_interactions,\n",
    "    '`The maximum number of user-article interactions by any 1 user is ______.`': max_views_by_user,\n",
    "    '`The most viewed article in the dataset was viewed _____ times.`': max_views,\n",
    "    '`The article_id of the most viewed article is ______.`': most_viewed_article_id,\n",
    "    '`The number of unique articles that have at least 1 rating ______.`': unique_articles,\n",
    "    '`The number of unique users in the dataset is ______`': unique_users,\n",
    "    '`The number of unique articles on the IBM platform`': total_articles\n",
    "}\n",
    "\n",
    "# Test your dictionary against the solution\n",
    "t.sol_1_test(sol_1_dict)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "cell_type": "code",
   "source": [
    "# No need to change the code here - this will be helpful for later parts of the notebook\n",
    "# Run this cell to map the user email to a user_id column and remove the email column\n",
    "\n",
    "def email_mapper(df):\n",
    "    coded_dict = dict()\n",
    "    cter = 1\n",
    "    email_encoded = []\n",
    "\n",
    "    for val in df['email']:\n",
    "        if val not in coded_dict:\n",
    "            coded_dict[val] = cter\n",
    "            cter+=1\n",
    "\n",
    "        email_encoded.append(coded_dict[val])\n",
    "    return email_encoded\n",
    "\n",
    "email_encoded = email_mapper(df)\n",
    "del df['email']\n",
    "df['user_id'] = email_encoded"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# write the cleaned-up file back to disk\n",
    "df.to_csv('../data/user_item_interactions_clean.csv')\n",
    "\n",
    "# show header\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"Rank\">Part II: Rank-Based Recommendations</a>\n",
    "\n",
    "We don't actually have ratings for whether a user liked an article or not.  We only know that a user has interacted with an article.  In these cases, the popularity of an article can really only be based on how often an article was interacted with.\n",
    "\n",
    "`1.` Function below returns the **n** top articles ordered with most interactions as the top."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "def get_top_articles(n, df=df):\n",
    "    \"\"\"\n",
    "    Find top n articles from interactions\n",
    "\n",
    "    INPUT:\n",
    "    n - (int) the number of top articles to return\n",
    "    df - (pandas dataframe) df as defined at the top of the notebook\n",
    "\n",
    "    OUTPUT:\n",
    "    top_articles - (list) A list of the top 'n' article titles\n",
    "    \"\"\"\n",
    "\n",
    "    top_article_titles = []\n",
    "    top_articles = get_top_article_ids(n, df)\n",
    "\n",
    "    for id in top_articles:\n",
    "        title = df.loc[df['article_id'] == id]['title'][:1].values[0]\n",
    "        top_article_titles.append(title)\n",
    "\n",
    "    # Return the top article titles from df (not df_content)\n",
    "    return top_article_titles\n",
    "\n",
    "\n",
    "def get_top_article_ids(n, df=df):\n",
    "    \"\"\"\n",
    "    Find the top n article id's from interactions\n",
    "\n",
    "    INPUT:\n",
    "    n - (int) the number of top articles to return\n",
    "    df - (pandas dataframe) df as defined at the top of the notebook\n",
    "\n",
    "    OUTPUT:\n",
    "    top_articles - (list) A list of the top 'n' article titles\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    top_articles = list(df.article_id.value_counts()[:n].index)\n",
    "\n",
    "    # Return the top article ids\n",
    "    return top_articles"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Udacity Evaluation"
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Test your function by returning the top 5, 10, and 20 articles\n",
    "top_5 = get_top_articles(5)\n",
    "top_10 = get_top_articles(10)\n",
    "top_20 = get_top_articles(20)\n",
    "\n",
    "# Test each of your three lists from above\n",
    "t.sol_2_test(get_top_articles)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Are all article_id's that have interactions also present in the content database ?"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "top_articles = np.array(df.article_id.value_counts().index)\n",
    "print('Number of unique articles that have interacgtions: {}'.format(len(top_articles)))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "articles_in_content = np.array(df_content.article_id.unique())\n",
    "print('Number of unique articles that have contents: {}'.format(len(articles_in_content)))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "top_articles_not_in_content = np.setdiff1d(\n",
    "    top_articles, articles_in_content)\n",
    "\n",
    "print('Number of articles that have interactions, '\n",
    "      'but are missing from the contents dataset: {}'.format(len(top_articles_not_in_content)))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Observation:**\n",
    "277 articles are missing in df_content that are present in df.\n",
    "\n",
    "We have interactions but not details about the content.\n",
    "Because this is a strict template provided by Udacity,\n",
    "I have to keep these records in this notebook,\n",
    "but will remove in them in the webapp as next step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"User-User\">Part III: User-User Based Collaborative Filtering</a>\n",
    "\n",
    "\n",
    "`1.` The function below will reformat the **df** dataframe to be shaped with users as the rows and articles as the columns.\n",
    "\n",
    "* Each **user** should only appear in each **row** once.\n",
    "\n",
    "\n",
    "* Each **article** should only show up in one **column**.  \n",
    "\n",
    "\n",
    "* **If a user has interacted with an article, then place a 1 where the user-row meets for that article-column**.  It does not matter how many times a user has interacted with the article, all entries where a user has interacted with an article should be a 1.  \n",
    "\n",
    "\n",
    "* **If a user has not interacted with an item, then place a zero where the user-row meets for that article-column**. \n",
    "\n",
    "Use the tests to make sure the basic structure of your matrix matches what is expected by the solution."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# create the user-article matrix with 1's and 0's\n",
    "\n",
    "def create_user_item_matrix(df):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    df - pandas dataframe with article_id, title, user_id columns\n",
    "\n",
    "    OUTPUT:\n",
    "    user_item - user item matrix\n",
    "\n",
    "    Description:\n",
    "    Return a matrix with user ids as rows and article ids on the columns\n",
    "    with 1 values where a user interacted with an article and a 0 otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    user_item = pd.crosstab(df.user_id, df.article_id)\n",
    "    user_item = user_item.where(user_item == 0, 1)\n",
    "\n",
    "    return user_item\n",
    "\n",
    "\n",
    "user_item = create_user_item_matrix(df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Udacity Evaluation"
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Tests: You should just need to run this cell.  Don't change the code.\n",
    "assert user_item.shape[0] == 5149,  \"Oops!  The number of users in the user-article matrix doesn't look right.\"\n",
    "assert user_item.shape[1] == 714,  \"Oops!  The number of articles in the user-article matrix doesn't look right.\"\n",
    "assert user_item.sum(axis=1)[1] == 36,  \"Oops!  The number of articles seen by user 1 doesn't look right.\"\n",
    "print(\"You have passed our quick tests!  Please proceed!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` The function below takes a user_id and provide an ordered list of the most similar users to that user (from most similar to least similar).\n",
    "\n",
    "The returned result should not contain the provided user_id, as we know that each user is similar to him/herself.\n",
    "Because the results for each user here are binary, it (perhaps) makes sense to compute similarity as the dot product of two users.\n",
    "\n",
    "Evaluate, which gives the best results, similarity using dot product vs KNN."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "def find_similar_users(user_id, user_item=user_item):\n",
    "    \"\"\"\n",
    "    Computes the similarity of every pair of users based on the dot product\n",
    "\n",
    "    INPUT:\n",
    "    user_id - (int) a user_id\n",
    "    user_item - (pandas dataframe) matrix of users by articles:\n",
    "                1's when a user has interacted with an article, 0 otherwise\n",
    "\n",
    "    OUTPUT:\n",
    "    similar_users - (list) an ordered list where the closest users\n",
    "                    (largest dot product users) are listed first\n",
    "    \"\"\"\n",
    "\n",
    "    # compute the similarity of each user to the provided user\n",
    "    input_user_series = user_item.loc[user_id]\n",
    "    dot_similarity = input_user_series.dot(np.transpose(user_item))\n",
    "\n",
    "    # sort by similarity\n",
    "    dot_similarity.sort_values(ascending=False, inplace=True)\n",
    "\n",
    "    # create list of just the ids\n",
    "    most_similar_users = list(dot_similarity.index)\n",
    "\n",
    "    # drop user id that is same as input\n",
    "    most_similar_users.remove(user_id)\n",
    "\n",
    "    # return a list of the users in order from most to least similar\n",
    "    return most_similar_users\n",
    "\n",
    "\n",
    "def find_similar_users_knn(user_id, user_item=user_item, n=10):\n",
    "    \"\"\"\n",
    "    Computes the similarity of every pair of users based on the KNN\n",
    "\n",
    "    INPUT:\n",
    "    user_id - (int) a user_id\n",
    "    user_item - (pandas dataframe) matrix of users by articles:\n",
    "                1's when a user has interacted with an article, 0 otherwise\n",
    "\n",
    "    OUTPUT:\n",
    "    similar_users - (list) an ordered list where the closest users\n",
    "                    (largest dot product users) are listed first\n",
    "    \"\"\"\n",
    "    knn = NearestNeighbors(n_neighbors=n+1,\n",
    "                           algorithm='auto',\n",
    "                           metric='cosine',\n",
    "                           n_jobs=-1)\n",
    "    knn.fit(user_item)\n",
    "    # contains indexes of similar users\n",
    "    neighbors = knn.kneighbors(user_item.loc[[user_id]],\n",
    "                               return_distance=False).squeeze()\n",
    "    # get user_ids\n",
    "    userids = user_item.iloc[neighbors].index.values.tolist()\n",
    "    userids.remove(user_id)\n",
    "    return userids"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Do a spot-check function using dot similarity\n",
    "print(\"The 10 most similar users to user 1 are: {}\".format(find_similar_users(1)[:10]))\n",
    "print(\"The 5 most similar users to user 3933 are: {}\".format(find_similar_users(3933)[:5]))\n",
    "print(\"The 3 most similar users to user 46 are: {}\".format(find_similar_users(46)[:3]))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Do a spot-check function using KNN\n",
    "print(\"The 10 most similar users to user 1 are: {}\".format(find_similar_users_knn(1, n=10)))\n",
    "print(\"The 5 most similar users to user 3933 are: {}\".format(find_similar_users_knn(3933, n=5)))\n",
    "print(\"The 3 most similar users to user 46 are: {}\".format(find_similar_users_knn(46, n=3)))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Conclusion:**\n",
    "KNN does not perform as well as the dot product.\n",
    "In the end, KNN is using dot product as well,\n",
    "but without considering magnitude.\n",
    "As magnitude is important for popularity,\n",
    "it's better to continue with only dot product to measure similarity."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "`3.` Now that we have a function that provides the most similar users to each user, lets use these users to find articles to recommend.  Below functions will return the articles to recommend to each user."
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "def get_article_names(article_ids, df=df):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    article_ids - (list) a list of article ids\n",
    "    df - (pandas dataframe) df as defined at the top of the notebook\n",
    "\n",
    "    OUTPUT:\n",
    "    article_names - (list) a list of article names associated with the list of article ids\n",
    "                    (this is identified by the title column)\n",
    "    \"\"\"\n",
    "\n",
    "    article_names = []\n",
    "\n",
    "    for id in article_ids:\n",
    "        id = int(id)\n",
    "        title = df.loc[df.article_id==id, 'title'].unique()\n",
    "        article_names.append(title[0])\n",
    "\n",
    "    # Return the article names associated with the list of article ids\n",
    "    return article_names\n",
    "\n",
    "\n",
    "def get_user_articles(user_id, user_item=user_item):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    user_id - (int) a user id\n",
    "    user_item - (pandas dataframe) matrix of users by articles: \n",
    "                1's when a user has interacted with an article, 0 otherwise\n",
    "\n",
    "    OUTPUT:\n",
    "    article_ids - (list) a list of the article ids seen by the user\n",
    "    article_names - (list) a list of article names associated with the list of article ids \n",
    "                    (this is identified by the doc_full_name column in df_content)\n",
    "\n",
    "    Description:\n",
    "    Provides a list of the article_ids and article titles that have been seen by a user\n",
    "    \"\"\"\n",
    "\n",
    "    # get a list of movies user has read\n",
    "    user_id = int(user_id)\n",
    "    docs_read = user_item.loc[user_id]\n",
    "    article_ids = list(map(str, docs_read[docs_read  == 1].index))\n",
    "    article_names = get_article_names(article_ids)\n",
    "\n",
    "    return article_ids, article_names # return the ids and names\n",
    "\n",
    "\n",
    "def get_user_articles2(user_id, user_item=user_item):\n",
    "    \"\"\"\n",
    "    Provides a list of the article_ids and article titles that have been seen by a user\n",
    "\n",
    "    Copy of get_user_articles but with user_id as an integer instead\n",
    "\n",
    "    INPUT:\n",
    "    user_id - (int) a user id\n",
    "    user_item - (pandas dataframe) matrix of users by articles:\n",
    "                1's when a user has interacted with an article, 0 otherwise\n",
    "\n",
    "    OUTPUT:\n",
    "    article_ids - (list) a list of the article ids seen by the user\n",
    "    article_names - (list) a list of article names associated with the list of article ids\n",
    "                    (this is identified by the doc_full_name column in df_content)\n",
    "    \"\"\"\n",
    "\n",
    "    # get a list of movies user has read\n",
    "    user_id = int(user_id)\n",
    "    docs_read = user_item.loc[user_id]\n",
    "    article_ids = list(docs_read[docs_read  == 1].index)\n",
    "    article_names = get_article_names(article_ids)\n",
    "\n",
    "    return article_ids, article_names # return the ids and names\n",
    "\n",
    "\n",
    "def user_user_recs(user_id, m=10, knn=False):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    user_id - (int) a user id\n",
    "    m - (int) the number of recommendations you want for the user\n",
    "\n",
    "    OUTPUT:\n",
    "    recs - (list) a list of recommendations for the user\n",
    "\n",
    "    Description:\n",
    "    Loops through the users based on closeness to the input user_id\n",
    "    For each user - finds articles the user hasn't seen before and provides them as recs\n",
    "    Does this until m recommendations are found\n",
    "\n",
    "    Notes:\n",
    "    Users who are the same closeness are chosen arbitrarily as the 'next' user\n",
    "\n",
    "    For the user where the number of recommended articles starts below m\n",
    "    and ends exceeding m, the last items are chosen arbitrarily\n",
    "    \"\"\"\n",
    "\n",
    "    docs_read = user_item.loc[user_id]\n",
    "    docs_read = docs_read[docs_read  == 1].index\n",
    "\n",
    "    # list of user id's that read similar documents to the requested user\n",
    "    if knn:\n",
    "        nearest_neighbors = find_similar_users_knn(user_id)[:m]\n",
    "    else:\n",
    "        nearest_neighbors = find_similar_users(user_id)[:m]\n",
    "\n",
    "    # reduce the user_item matrix to only nearest neighbors\n",
    "    neighbors_docs = user_item.reindex(nearest_neighbors)\n",
    "\n",
    "    # get the articles read the most by neighbors with highest score\n",
    "    recs = neighbors_docs.sum().sort_values(ascending=False)\n",
    "\n",
    "    # drop articles read already\n",
    "    recs = recs.drop(docs_read, errors='ignore')\n",
    "\n",
    "    # get titles\n",
    "    titles = get_article_names(recs[:m].index)\n",
    "\n",
    "    return titles # return your recommendations for this user_id"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Udacity evaluation"
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Test your functions here - No need to change this code - just run this cell\n",
    "assert set(get_article_names(['1024', '1176', '1305', '1314', '1422', '1427'])) == set(['using deep learning to reconstruct high-resolution audio', 'build a python app on the streaming analytics service', 'gosales transactions for naive bayes model', 'healthcare python streaming application demo', 'use r dataframes & ibm watson natural language understanding', 'use xgboost, scikit-learn & ibm watson machine learning apis']), \"Oops! Your the get_article_names function doesn't work quite how we expect.\"\n",
    "assert set(get_article_names(['1320', '232', '844'])) == set(['housing (2015): united states demographic measures','self-service data preparation with ibm data refinery','use the cloudant-spark connector in python notebook']), \"Oops! Your the get_article_names function doesn't work quite how we expect.\"\n",
    "assert set(get_user_articles(20)[0]) == set(['1320', '232', '844'])\n",
    "assert set(get_user_articles(20)[1]) == set(['housing (2015): united states demographic measures', 'self-service data preparation with ibm data refinery','use the cloudant-spark connector in python notebook'])\n",
    "assert set(get_user_articles(2)[0]) == set(['1024', '1176', '1305', '1314', '1422', '1427'])\n",
    "assert set(get_user_articles(2)[1]) == set(['using deep learning to reconstruct high-resolution audio', 'build a python app on the streaming analytics service', 'gosales transactions for naive bayes model', 'healthcare python streaming application demo', 'use r dataframes & ibm watson natural language understanding', 'use xgboost, scikit-learn & ibm watson machine learning apis'])\n",
    "print(\"If this is all you see, you passed all of our tests!  Nice job!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4.` Now lets improve the consistency of the **user_user_recs** function from above.\n",
    "\n",
    "* Instead of arbitrarily choosing when we obtain users who are all the same closeness to a given user - choose the users that have the most total article interactions before choosing those with fewer article interactions.\n",
    "\n",
    "* Instead of arbitrarily choosing articles from the user where the number of recommended articles starts below m and ends exceeding m, choose articles with the articles with the most total interactions before choosing those with fewer total interactions. This ranking should be  what would be obtained from the **top_articles** function you wrote earlier."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "top_article_interactions = df.user_id.value_counts()\n",
    "top_article_interactions"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "def get_top_sorted_users(user_id, df=df, user_item=user_item, knn=False):\n",
    "    \"\"\"\n",
    "    Get most similar users\n",
    "\n",
    "    INPUT:\n",
    "    user_id - (int)\n",
    "    df - (pandas dataframe) df as defined at the top of the notebook \n",
    "    user_item - (pandas dataframe) matrix of users by articles: \n",
    "            1's when a user has interacted with an article, 0 otherwise\n",
    "    \n",
    "            \n",
    "    OUTPUT:\n",
    "    neighbors_df - (pandas dataframe) a dataframe with:\n",
    "                    neighbor_id - is a neighbor user_id\n",
    "                    similarity - measure of the similarity of each user to the provided user_id\n",
    "                    num_interactions - the number of articles viewed by the user - if a u\n",
    "                    \n",
    "    Other Details - sort the neighbors_df by the similarity and then by number of interactions where \n",
    "                    highest of each is higher in the dataframe\n",
    "     \n",
    "    \"\"\"\n",
    "\n",
    "    dot_similarity = pd.DataFrame()\n",
    "\n",
    "    # list of user id's that read similar documents to the requested user\n",
    "    if knn:\n",
    "        neighbors = find_similar_users_knn(user_id)\n",
    "        knn = NearestNeighbors(n_neighbors=100, algorithm='auto', metric='cosine', n_jobs=-1)\n",
    "        knn.fit(user_item)\n",
    "        # contains indexes of similar users\n",
    "        distance, neighbors = knn.kneighbors(user_item.loc[[user_id]], return_distance=True)\n",
    "        # get user_ids\n",
    "        userids = user_item.iloc[neighbors.squeeze()].index.values.tolist()\n",
    "        similarity = pd.DataFrame({'similarity': distance.squeeze()}, index=userids)\n",
    "\n",
    "    else:\n",
    "\t    # compute the similarity of each user to the provided user\n",
    "        input_user_series = user_item.loc[user_id]\n",
    "        similarity = input_user_series.dot(np.transpose(user_item))\n",
    "\n",
    "    # sort user_ids by top interactions\n",
    "    top_article_interactions = df.user_id.value_counts()\n",
    "\n",
    "    # merge and sort similarities and interactions\n",
    "    neighbors_df = pd.concat([similarity, top_article_interactions], axis=1)\n",
    "    neighbors_df.columns = ['similarity', 'num_interactions']\n",
    "    neighbors_df.drop(user_id, axis=0, inplace=True)\n",
    "    neighbors_df.sort_values(by=['similarity', 'num_interactions'], ascending=False, inplace=True)\n",
    "\n",
    "    return neighbors_df # Return the dataframe specified in the doc_string\n",
    "\n",
    "\n",
    "def user_user_recs_part2(user_id, m=10, knn=False):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    user_id - (int) a user id\n",
    "    m - (int) the number of recommendations you want for the user\n",
    "    \n",
    "    OUTPUT:\n",
    "    recs - (list) a list of recommendations for the user by article id\n",
    "    rec_names - (list) a list of recommendations for the user by article title\n",
    "    \n",
    "    Description:\n",
    "    Loops through the users based on closeness to the input user_id\n",
    "    For each user - finds articles the user hasn't seen before and provides them as recs\n",
    "    Does this until m recommendations are found\n",
    "    \n",
    "    Notes:\n",
    "    * Choose the users that have the most total article interactions \n",
    "    before choosing those with fewer article interactions.\n",
    "\n",
    "    * Choose articles with the articles with the most total interactions \n",
    "    before choosing those with fewer total interactions. \n",
    "   \n",
    "    \"\"\"\n",
    "\n",
    "    recs = []\n",
    "\n",
    "    # Get articles input user has read\n",
    "    input_id, input_titles = get_user_articles2(user_id)\n",
    "\n",
    "    # get top article interactions\n",
    "    top_article_interactions = df.article_id.value_counts()\n",
    "\n",
    "    # get nearest neighbors\n",
    "    neighbors_df = get_top_sorted_users(user_id, df=df)\n",
    "    # print(neighbors_df.head())\n",
    "\n",
    "    for neighbor in neighbors_df.index:\n",
    "\n",
    "        # get articles the neighbor has read\n",
    "        id, titles = get_user_articles2(neighbor)\n",
    "\n",
    "        # remove articles input user has read\n",
    "        new_recs = np.setdiff1d(id, input_id)\n",
    "\n",
    "        # sort articles of the neighbor by number of interactions\n",
    "        recs_to_add = top_article_interactions.loc[new_recs].sort_values(ascending=False).index\n",
    "\n",
    "        # add articles to our recommendation list\n",
    "        recs.extend(recs_to_add)\n",
    "\n",
    "        # stop if recommendations exceed the number of required recommendations\n",
    "        if len(recs) >= m:\n",
    "            break\n",
    "\n",
    "    # Select only the top_n\n",
    "    recs = recs[:m]\n",
    "\n",
    "    # get the list of recommended article titles\n",
    "    rec_names = get_article_names(recs)\n",
    "\n",
    "    return recs, rec_names"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Let's try to test this function\n",
    "# Actual documents read by user 4999, it's easy to judge manually as the user read only 3 documents\n",
    "get_user_articles2(4999)[1]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# get_top_sorted_users(4999, knn=True)\n",
    "user_user_recs_part2(4999, knn=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Conclusion:\n",
    "Once again, we see that **dot product** is a better metric than KNN for this use case.\n",
    "KNN using cosine is actually also using dot product but is normalized by magnitude.\n",
    "In our use case, magnitude gives better results as popularity matters."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Quick spot check - don't change this code - just use it to test your functions\n",
    "rec_ids, rec_names = user_user_recs_part2(20, 10)\n",
    "print(\"The top 10 recommendations for user 20 are the following article ids:\")\n",
    "print(rec_ids)\n",
    "print()\n",
    "print(\"The top 10 recommendations for user 20 are the following article names:\")\n",
    "print(rec_names)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`5.` Use your functions from above to correctly fill in the solutions to the dictionary below.  Then test your dictionary against the solution.  Provide the code you need to answer each following the comments below."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Udacity Evaluation"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "get_top_sorted_users(1).index.values.tolist()[:1]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "get_top_sorted_users(131).index.values.tolist()[:10]",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### Tests with a dictionary of results\n",
    "user1_most_sim =  3933 # Find the user that is most similar to user 1\n",
    "user131_10th_sim = 242 # Find the 10th most similar user to user 131"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Dictionary Test Here\n",
    "sol_5_dict = {\n",
    "    'The user that is most similar to user 1.': user1_most_sim, \n",
    "    'The user that is the 10th most similar to user 131': user131_10th_sim,\n",
    "}\n",
    "\n",
    "t.sol_5_test(sol_5_dict)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "`6.`"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For a new user,\n",
    "collaborative filtering does not work well to make predictions,\n",
    "as it's using historical data to find similar users to make predictions,\n",
    "which is absent for new users.\n",
    "\n",
    "I propose two options for recommendations to new users:\n",
    "1) Ranked-based recommendations <br>\n",
    "like in Part II in function\n",
    "`get_top_articles` to propose the most popular articles.\n",
    "\n",
    "2) Content-based recommendations <br>\n",
    "Use NLP techniques\n",
    "to propose articles to the user based on an input search criteria\n",
    "provided by the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "`7.` Use existing functions, and provide the top 10 recommended articles you would provide for the a new user."
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "new_user = '0.0'\n",
    "\n",
    "# What would we recommend be for new user '0.0'?\n",
    "# As a new user, they have no observed articles.\n",
    "# Provide a list of the top 10 article ids you would give to.\n",
    "new_user_recs = get_top_article_ids(n=10)\n",
    "# convert to a floated string to match Udacity testing\n",
    "new_user_recs = [str(float(x)) for x in new_user_recs]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "assert set(new_user_recs) == set(['1314.0','1429.0','1293.0','1427.0','1162.0','1364.0','1304.0','1170.0','1431.0','1330.0']), \"Oops!  It makes sense that in this case we would want to recommend the most popular articles, because we don't know anything about these users.\"\n",
    "\n",
    "print(\"That's right!  Nice job!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"Content-Recs\">Part IV: Content Based Recommendations</a>\n",
    "\n",
    "Another method we can use to make recommendations, is\n",
    "to recommend documents with the highest text-based similarity to an input search term.\n",
    "\n",
    "Use NLP techniques to convert document texts into words.\n",
    "\n",
    "Merge `doc_body`, `doc_description`\n",
    "and `doc_full_name` to improve search term performance."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# first let's enhance df_content\n",
    "# for content-based recommendations,\n",
    "# merge title, description, and body fields into a new field\n",
    "df_content['doc_body_all'] = (\n",
    "    df_content[df_content.columns[:]].apply(\n",
    "        lambda x: ','.join(x.astype(str)), axis=1))\n",
    "\n",
    "df_content.columns"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def tokenize(text):\n",
    "    \"\"\" Summarize text into words whilst cleaning it up\n",
    "\n",
    "    Most important functions:\n",
    "    - Summarize url links starting with http or www to a common phrase 'url\n",
    "    - Summarize email addresses to a common phrase 'email'\n",
    "    - Get rid of new lines `\\n'\n",
    "    - Remove all words that are just numbers\n",
    "    - Remove all words that contain numbers\n",
    "    - Cleanup basic punctuation like '..', '. .'\n",
    "    - Remove punctuation\n",
    "    - Remove words that are just 1 character long after removing punctuation\n",
    "    - Use lemmatization to bring words to the base\n",
    "\n",
    "    Args:\n",
    "        text: string, Text sentences to be split into words\n",
    "\n",
    "    Return:\n",
    "        clean_tokens: list, List containing most crucial words\n",
    "    \"\"\"\n",
    "\n",
    "    # Replace urls starting with 'https' with placeholder\n",
    "    url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    # replace urls with a common keyword\n",
    "    detected_urls = re.findall(url_regex, text)\n",
    "    for url in detected_urls:\n",
    "        text = text.replace(url, 'url')\n",
    "\n",
    "    # Replace urls starting with 'www' with placeholder\n",
    "    url_regex = 'www.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    detected_urls = re.findall(url_regex, text)\n",
    "    for url in detected_urls:\n",
    "        text = text.replace(url, 'url')\n",
    "\n",
    "    # replace emails with placeholder\n",
    "    email_regex = '([A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,})'\n",
    "    detected_emails = re.findall(email_regex, text)\n",
    "    for email in detected_emails:\n",
    "        text = text.replace(email, 'email')\n",
    "\n",
    "    # replace newlines, which can negatively affect performance.\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = text.replace(\"\\r\", \" \")\n",
    "    text = text.replace(\"..\", \".\")\n",
    "    text = text.replace(\". .\", \".\")\n",
    "    text = text.replace(\" ,.\", \".\")\n",
    "\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # normalize text by removing punctuation, remove case and strip spaces\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
    "    text = text.lower().strip()\n",
    "\n",
    "    # remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    #  split sentence into words\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords, e.g. 'the', 'a',\n",
    "    tokens = [w for w in tokens if w not in stopwords.words(\"english\")]\n",
    "\n",
    "    # take words to their core, e.g. children to child, organizations to organization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        clean_tok = lemmatizer.lemmatize(tok, wordnet.VERB)\n",
    "        # ignore tokens that have only 1 character or contains numbers\n",
    "        if len(clean_tok) >= 2 & clean_tok.isalpha():\n",
    "            clean_tokens.append(clean_tok)\n",
    "\n",
    "    return clean_tokens"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_word_count_matrix(df=df_content.copy(deep=True), column='doc_body'):\n",
    "    \"\"\"\n",
    "    Create a word count matrix for a dataframe column containing text\n",
    "\n",
    "    Input:\n",
    "    df: pandas dataframe containing document texts, with identifier 'article_id'\n",
    "    column: string -> column to convert to word counts\n",
    "\n",
    "    Output:\n",
    "    tfidf_df: pandas dataframe containing the word count matrix\n",
    "\n",
    "    \"\"\"\n",
    "    df.set_index('article_id', inplace=True)\n",
    "\n",
    "    # Create a word count matrix by article\n",
    "    tfidf_vectorizer = TfidfVectorizer(min_df=3,\n",
    "                                       max_df=0.6,\n",
    "                                       tokenizer=tokenize,\n",
    "                                       token_pattern=None,\n",
    "                                       max_features=5000)\n",
    "    vectorized_data = tfidf_vectorizer.fit_transform(df[column])\n",
    "    tfidf_df = pd.DataFrame(vectorized_data.toarray(),\n",
    "                            columns=tfidf_vectorizer.get_feature_names_out(),\n",
    "                            index=df.index)\n",
    "    return tfidf_df, tfidf_vectorizer"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tfidf_df, tfidf_vectorizer = create_word_count_matrix(df_content.copy(deep=True), column='doc_body_all')\n",
    "tfidf_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "cell_type": "code",
   "source": [
    "def print_content_recommendations(ids, df=df_content):\n",
    "    \"\"\"\n",
    "    Print document titles and descriptions based on a list of recommended article_ids\n",
    "\n",
    "    Input:\n",
    "    ids: list -> list of input article ids\n",
    "    df: pandas dataframe containing document texts, with identifier column 'article_id'\n",
    "\n",
    "    Output:\n",
    "    Print article id, document name and description\n",
    "    \"\"\"\n",
    "\n",
    "    print('Possible recommendations are:')\n",
    "    for id in ids:\n",
    "        doc_full_name = df.loc[df['article_id'] == id, ['doc_full_name', 'doc_description']].values[0]\n",
    "        print('{:>6}: {:>12}.  {:>12}'.format(id, doc_full_name[0], doc_full_name[1]))\n",
    "\n",
    "\n",
    "def make_content_recs(input_search_text, tfidf_vectorizer=tfidf_vectorizer, tfidf_df=tfidf_df, n=10, df=df):\n",
    "    \"\"\"\n",
    "    Content-based recommendations based on text-based similarity\n",
    "\n",
    "    User input any search text\n",
    "\n",
    "    INPUT:\n",
    "    input_search_text: string, any text a user input to search for documents\n",
    "\n",
    "    OUTPUT:\n",
    "    content_ids: list, List containing article_id recommendations\n",
    "    content_descriptions: list, List containing article descriptions\n",
    "\n",
    "    \"\"\"\n",
    "    input_search_text = tfidf_vectorizer.transform([input_search_text])\n",
    "    similarity = cosine_similarity(input_search_text, tfidf_df)\n",
    "    cosine_similarity_df = pd.DataFrame(similarity, columns=tfidf_df.index)\n",
    "    ids = cosine_similarity_df.loc[0].sort_values(ascending=False).index.values[:n].tolist()\n",
    "    print_content_recommendations(ids)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "make_content_recs('Why machine learning?')",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "make_content_recs('Gradient Boosting')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "`2.`"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation of your content-based recommendation system.**\n",
    "\n",
    "The content-based recommendation system is using a text-based similarity search between user input and the text body of each document.\n",
    "Documents are recommended\n",
    "where the most words are matching the input criteria.\n",
    "The following steps are followed:\n",
    "\n",
    "1) Create a tokenizer to clean-up the text, e.g. lower case, remove accents, stopwords, line feeds, numbers, etc.\n",
    "2) Use the tokenizer in sklearn's `TdidfVectorizer()` feature extraction function to create a Document-Term matrix, with words and their frequency as columns and article_id as index.\n",
    "3) Convert the user input search text using the same model as fitted in step 2. This creates a comparable document-term matrix as above with the same shape.\n",
    "4) Use cosine similarity to compare the user input with the documents to find the most similar term frequencies.\n",
    "5) Propose the top n documents with the highest cosine score to the user.\n",
    "\n",
    "The solution is working pretty well,\n",
    "however,\n",
    "it's based on a static word count similarity\n",
    "that does not understand semantics or context.\n",
    "More modern AI LLM, and embeddings would allow a much smarter and more modern recommendation system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "`3.` Make some more recommendations"
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# make recommendations for a brand-new user\n",
    "make_content_recs('XGBOOST')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# make a recommendation for a user who only has interacted with article id '1427.0'\n",
    "# get the title of the article that was read\n",
    "title = df.loc[df['article_id'] == 1427, ['title']][:1].squeeze()\n",
    "print('Title the user has read is: {}\\n'.format(title))\n",
    "\n",
    "# use this title to get similar articles\n",
    "make_content_recs(title)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"Matrix-Fact\">Part V: Matrix Factorization</a>\n",
    "\n",
    "In this part of the notebook, we will use matrix factorization to make article recommendations to the users on the IBM Watson Studio platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "`2.` In this situation, you can use Singular Value Decomposition on the user-item matrix.  Perform SVD, and explain why this is different than in the lessons."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# calculate svd predictions as per Udacity lesson,\n",
    "# using my own user_item matrix from above.\n",
    "u, s, vt = np.linalg.svd(np.array(user_item))\n",
    "u.shape, s.shape, vt.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the Udacity lessons,\n",
    "we use SVDs\n",
    "to predict the ratings of missing values,\n",
    "where in this exercise we are not dealing with missing values but binary indicators\n",
    "showing if a user read a document or not.\n",
    "\n",
    "SVD in this use case could make sense\n",
    "to investigate latent features to detect patterns in the data,\n",
    "reduce dimensionality to improve overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "`3.` Now for the tricky part, how do we choose the number of latent features to use?  Running the below cell, we can see that as the number of latent features increases, we get a lower error rate on making predictions for the 1 and 0 values in the user-item matrix.  Run the cell below to get an idea of how the accuracy improves as we increase the number of latent features."
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "num_latent_feats = np.arange(10,700+10,20)\n",
    "sum_errs = []\n",
    "\n",
    "for k in num_latent_feats:\n",
    "    # restructure with k latent features\n",
    "    s_new, u_new, vt_new = np.diag(s[:k]), u[:, :k], vt[:k, :]\n",
    "    \n",
    "    # take dot product\n",
    "    user_item_est = np.around(np.dot(np.dot(u_new, s_new), vt_new))\n",
    "    \n",
    "    # compute error for each prediction to actual value\n",
    "    diffs = np.subtract(user_item, user_item_est)\n",
    "    \n",
    "    # total errors and keep track of them\n",
    "    err = np.sum(np.sum(np.abs(diffs), axis=0), axis=0)\n",
    "    sum_errs.append(err)\n",
    "    \n",
    "\n",
    "max_error = user_item.shape[0] * user_item.shape[1]\n",
    "plt.plot(num_latent_feats, 1 - np.array(sum_errs)/max_error)\n",
    "plt.xlabel('Number of Latent Features')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs. Number of Latent Features')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4.` From the above, we can't really be sure how many features to use, because simply having a better way to predict the 1's and 0's of the matrix doesn't exactly give us an indication of if we are able to make good recommendations.  Instead, we might split our dataset into a training and test set of data, as shown in the cell below.  \n",
    "\n",
    "Using the code from question 3 to understand the impact on accuracy of the training and test sets of data with different numbers of latent features. Using the split below:\n",
    "\n",
    "* How many users can we make predictions for in the test set?  \n",
    "* How many users are we not able to make predictions for because of the cold start problem?\n",
    "* How many articles can we make predictions for in the test set?  \n",
    "* How many articles are we not able to make predictions for because of the cold start problem?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "df_train = df.head(40000)\n",
    "df_test = df.tail(5993)\n",
    "\n",
    "def create_test_and_train_user_item(df_train, df_test, correct_history=True):\n",
    "    \"\"\"\n",
    "    Create training and testing user-item matrix\n",
    "\n",
    "    INPUT:\n",
    "    df_train - training dataframe\n",
    "    df_test - test dataframe\n",
    "    \n",
    "    OUTPUT:\n",
    "    user_item_train - a user-item matrix of the training dataframe \n",
    "                      (unique users for each row and unique articles for each column)\n",
    "    user_item_test - a user-item matrix of the testing dataframe \n",
    "                    (unique users for each row and unique articles for each column)\n",
    "    test_idx - all of the test user ids\n",
    "    test_arts - all of the test article ids\n",
    "    \n",
    "    \"\"\"\n",
    "    user_item_train = create_user_item_matrix(df_train)\n",
    "    user_item_test = create_user_item_matrix(df_test)\n",
    "    test_idx  = user_item_test.index.values.tolist()\n",
    "    test_arts = user_item_test.columns.values.tolist()\n",
    "\n",
    "    # find common users and articles\n",
    "    common_users = np.intersect1d(user_item_train.index, user_item_test.index)\n",
    "    common_articles = np.intersect1d(user_item_train.columns, user_item_test.columns)\n",
    "\n",
    "    # keep only common users and articles in the testing dataset\n",
    "    user_item_test = user_item_test.loc[common_users, common_articles]\n",
    "\n",
    "    # correct the history in the test dataset\n",
    "    # If a user read a document in the past in the training dataset,\n",
    "    # set the document to read also in the test dataset.\n",
    "    if correct_history:\n",
    "        for user in common_users:\n",
    "            for article in common_articles:\n",
    "                if (user_item_train.loc[user, article] == 1) & (user_item_test.loc[user, article] == 0):\n",
    "                    user_item_test.loc[user, article] = 1\n",
    "\n",
    "    return user_item_train, user_item_test, test_idx, test_arts\n",
    "\n",
    "\n",
    "user_item_train, user_item_test, test_idx, test_arts = create_test_and_train_user_item(df_train, df_test)\n",
    "user_item_train.shape, user_item_test.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. How many users can we make predictions for in the test set?\n",
    "# How many users in the test dataset are also in the training dataset?\n",
    "# (We can only make predictions for these users)\n",
    "pred_user_ids = user_item_train.loc[user_item_train.index.isin(test_idx)].index.values.tolist()\n",
    "print('We can predict for {} users: {}'.format(len(pred_user_ids), pred_user_ids))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "common_users = np.intersect1d(user_item_train.index, user_item_test.index)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "common_articles = np.intersect1d(user_item_train.columns, user_item_test.columns)\n",
    "len(common_articles)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. How many users in the test set are we not able\n",
    "# to make predictions for because of the cold start problem?\n",
    "# How many users are in the test dataset\n",
    "len(test_idx) - len(pred_user_ids)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. How many articles can we make predictions for in the test set?\n",
    "# How many articles in the test dataset are also in the training dataset?\n",
    "# (We can only make predictions for these)\n",
    "user_item_train.loc[:, test_arts].shape[1]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. How many articles in the test set are we not able\n",
    "# to make predictions for because of the cold start problem?\n",
    "len(test_arts) - user_item_train.loc[:, test_arts].shape[1]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Udacity Evaluation"
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Replace the values in the dictionary below\n",
    "a = 662 \n",
    "b = 574 \n",
    "c = 20 \n",
    "d = 0 \n",
    "\n",
    "sol_4_dict = {\n",
    "    'How many users can we make predictions for in the test set?': c,\n",
    "    'How many users in the test set are we not able to make predictions for because of the cold start problem?': a,\n",
    "    'How many articles can we make predictions for in the test set?': b,\n",
    "    'How many articles in the test set are we not able to make predictions for because of the cold start problem?': d,\n",
    "}\n",
    "\n",
    "t.sol_4_test(sol_4_dict)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`5.` Use the **user_item_train** dataset from above to find U, S, and V transpose using SVD. Then find the subset of rows in the **user_item_test** dataset that you can predict using this matrix decomposition with different numbers of latent features to see how many features makes sense to keep based on the accuracy on the test data. This will require combining what was done in questions `2` - `4`.\n",
    "\n",
    "We can use the cells below to explore how well SVD works towards making predictions for recommendations on the test data."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def determine_latent_features(df_train, df_test, test_idx=test_idx, test_arts=test_arts):\n",
    "    \"\"\"\n",
    "    Determine how many SVD latent features maximizes accuracy and MSE the best.\n",
    "\n",
    "    Use the train dataset to find U, S, and V transpose using SVD.\n",
    "    Find the subset of rows in the test dataset\n",
    "    that you can predict, that are also present in the training dataset.\n",
    "    Use the matrix decomposition with different numbers of latent features\n",
    "    to see how many features make sense\n",
    "    to keep based on the accuracy and mse on the test data.\n",
    "\n",
    "    INPUT:\n",
    "    df_train - training dataframe\n",
    "    df_test - test dataframe\n",
    "    test_idx - test user ids present in both test and training datasets (rows)\n",
    "    test_arts - test articles ids present in both test and training datasets (columns)\n",
    "\n",
    "    OUTPUT:\n",
    "    num_latent_feats: array of k latent features used for prediction\n",
    "    sum_errs:\n",
    "    list containing accuracy by nr of latent features used\n",
    "    mse: list containing mse by nr of latent features used\n",
    "    \"\"\"\n",
    "\n",
    "    num_latent_feats = np.arange(10, len(common_articles)+10, 50)\n",
    "    train_acc_errs = []\n",
    "    test_acc_errs = []\n",
    "    train_mse = []\n",
    "    test_mse = []\n",
    "\n",
    "    for k in num_latent_feats:\n",
    "\n",
    "        # TRAINING PREDICTIONS\n",
    "        # Decompose the matrix using training dataset with SVD with k latent features\n",
    "        u_train, s_train, vt_train = svds(np.array(df_train), k=k)\n",
    "\n",
    "        # correct shape of s (latent features)\n",
    "        s_train = np.diag(s_train)\n",
    "\n",
    "        # predict if user will read the article using dot product\n",
    "        training_prediction = np.around(np.dot(np.dot(u_train, s_train), vt_train), 0)\n",
    "\n",
    "        # compute error for each prediction to actual value\n",
    "        train_difference = np.subtract(df_train, training_prediction)\n",
    "\n",
    "        # total errors and keep track of them\n",
    "        # accuracy\n",
    "        train_acc_err = np.sum(np.sum(np.abs(train_difference), axis=0), axis=0)\n",
    "        train_acc_err = 1 - (train_acc_err / (training_prediction.shape[0]*training_prediction.shape[1]))\n",
    "        train_acc_errs.append(train_acc_err)\n",
    "\n",
    "        # mse error\n",
    "        train_mse_err = mean_squared_error(df_train, training_prediction)\n",
    "        train_mse.append(train_mse_err)\n",
    "\n",
    "        # TESTING PREDICTIONS\n",
    "        # filter training dataset predictions to only common users and articles\n",
    "        # (one could also directly just filter the training predictions)\n",
    "        row_idx = df_train.index.isin(test_idx)\n",
    "        col_idx = df_train.columns.isin(test_arts)\n",
    "        u_test = u_train[row_idx, :]\n",
    "        vt_test = vt_train[:, col_idx]\n",
    "        testing_prediction = np.around(np.dot(np.dot(u_test, s_train), vt_test), 0)\n",
    "        test_difference = np.subtract(df_test, testing_prediction)\n",
    "\n",
    "        # total errors and keep track of them\n",
    "        # accuracy\n",
    "        test_acc_err = np.sum(np.sum(np.abs(test_difference), axis=0), axis=0)\n",
    "        test_acc_err = 1 - (test_acc_err / (testing_prediction.shape[0] * testing_prediction.shape[1]))\n",
    "        test_acc_errs.append(test_acc_err)\n",
    "\n",
    "        # mse\n",
    "        test_mse_err = mean_squared_error(df_test, testing_prediction)\n",
    "        test_mse.append(test_mse_err)\n",
    "\n",
    "    return num_latent_feats, train_acc_errs, test_acc_errs, train_mse, test_mse\n",
    "\n",
    "num_latent_feats, train_acc_errs, test_acc_errs, train_mse, test_mse = determine_latent_features(user_item_train, user_item_test, test_idx)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.plot(num_latent_feats, train_acc_errs, label='training')\n",
    "plt.plot(num_latent_feats, test_acc_errs, label='testing')\n",
    "plt.xlabel('Number of Latent Features')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs. Number of Latent Features')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plt.plot(num_latent_feats, 1 - np.array(sum_errs)/df.shape[0])\n",
    "plt.plot(num_latent_feats, train_mse, label='training')\n",
    "plt.plot(num_latent_feats, test_mse, label='testing')\n",
    "plt.xlabel('Number of Latent Features')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('MSE vs. Number of Latent Features')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": "`6.`"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Summary of Matrix Factorization:**\n",
    "\n",
    "The following steps were followed to predict which documents a user might like:\n",
    "**1. Split data into training and testing datasets.**\n",
    "*  Filter the testing dataset to contain only common users and articles that are present in both the training and testing datasets.\n",
    "*  Correct historical data in the testing dataset. If a user reads a document in the past in the training dataset, set it to read in the testing dataset too.\n",
    "\n",
    "**2.  Prediction** <br>\n",
    "* The full training dataset is used with all users and articles\n",
    "to find all latent features\n",
    "that describes the underlying relationships and patterns in the data.\n",
    "\n",
    "**3.  Evaluate** <br>\n",
    "* Once we established all the relationships during the training steps,\n",
    "we can use it to predict more accurately user preferences using the test dataset.\n",
    "* The training predictions are filtered\n",
    "to only include the common users and articles in both datasets.\n",
    "These predictions are compared to the test dataset to calculate accuracy and MSE error.\n",
    "\n",
    "<br>**Conclusions:**\n",
    "\n",
    "* Both the training and testing predictions show improvement in accuracy\n",
    "as the latent features increase.\n",
    "* Note: If we do not correct historical data in the test dataset,\n",
    "it gives a false impression that the accuracy goes down, because\n",
    "read documents in the past show as '0' and are compared against predicted values of '1'.\n",
    "* MSE error goes down as latent features increase.\n",
    "* 300 Latent features looks like a good feature selection to use,\n",
    "as improvement slows down after this.\n",
    "* We could only make recommendations for 20 users, which is quite low.\n",
    "It would be more ideal to do evaluations on a larger group of users.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### **How to proof if recommendations are an improvement to how users find articles?**\n",
    "\n",
    "Use A/B testing to compare recommendations with users' natural behavior.\n",
    "\n",
    "**Metrics**: Average number of documents interacted with per user\n",
    "\n",
    "Divide users with low interactions randomly into two groups.\n",
    "Group A receives no recommendations and group B receives recommendations.\n",
    "\n",
    "In Part I above during exploratory data analysis,\n",
    "we observed\n",
    "that users up to the 75th percentile only interacted with nine documents or less.\n",
    "\n",
    "Can we improve this interaction?\n",
    "\n",
    "Filter data using the first 75th percentile only,\n",
    "to exclude users that interact frequently to avoid bias.\n",
    "Divide these filtered users into each group randomly.\n",
    "\n",
    "<br>**Hypothesis test for Invariant Metric:**<br>\n",
    "Before we start our experiment,\n",
    "make sure the two groups have users who behave similarly.\n",
    "Make sure the average interactions per user are not significantly different between the two groups.\n",
    "Significance level is 0.05\n",
    "\n",
    "$H_0: A_{avg_interactions} - B_{avg_interactions} = 0$\n",
    "\n",
    "$H_1: B_{avg_interactions} - A_{avg_interactions} <> 0$\n",
    "\n",
    "\n",
    "<br>**Hypothesis to test recommendation improvement:**<br>\n",
    "Ideally we would like\n",
    "to see an improvement in interactions in group B\n",
    "that received recommendations.\n",
    "The significance level is 0.05\n",
    "\n",
    "<br>$H_0: B_{interactions} - A_{interactions} <= 0$\n",
    "\n",
    "$H_1: B_{interactions} - A_{interactions} > 0$\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# convert notebook to html\n",
    "import os\n",
    "os.system('jupyter nbconvert --to html Recommendations_with_IBM.ipynb')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:recommendations] *",
   "language": "python",
   "name": "conda-env-recommendations-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
